#!/usr/bin/env python3
"""
Qwen3 æ¨¡å‹è°ƒç”¨è„šæœ¬ - 128GBå†…å­˜ä¼˜åŒ–ç‰ˆ
"""

from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
import argparse
import os
import torch
import gc

def load_model_and_tokenizer(model_name):
    model_dir = os.path.join(os.getcwd(), "models", model_name.replace("/", "-"))
    os.makedirs(model_dir, exist_ok=True)
    
    print(f"æ¨¡å‹å°†ä¿å­˜åˆ°: {model_dir}")
    
    # æ£€æŸ¥ç³»ç»Ÿèµ„æº
    if torch.cuda.is_available():
        for i in range(torch.cuda.device_count()):
            gpu_mem = torch.cuda.get_device_properties(i).total_memory / 1e9
            print(f"GPU {i}: {gpu_mem:.1f}GB")
    
    import psutil
    total_mem = psutil.virtual_memory().total / 1e9
    available_mem = psutil.virtual_memory().available / 1e9
    print(f"ç³»ç»Ÿå†…å­˜: {total_mem:.1f}GB (å¯ç”¨: {available_mem:.1f}GB)")
    
    # æ¸…ç†å†…å­˜
    gc.collect()
    torch.cuda.empty_cache()
    
    # åŠ è½½tokenizer
    tokenizer = AutoTokenizer.from_pretrained(
        model_name,
        cache_dir=model_dir,
        trust_remote_code=True
    )
    
    loading_strategies = []
    
    # ç­–ç•¥1ï¼šFP16ç›´æ¥åŠ è½½
    if not "Next" in model_name:
        loading_strategies.append({
            "name": "åŒGPU FP16ç›´æ¥åŠ è½½",
            "params": {
                "dtype": torch.float16,
                "device_map": "auto",
                "cache_dir": model_dir,
                "low_cpu_mem_usage": True,
                "trust_remote_code": True,
                "max_memory": {
                    0: "44GB",
                    1: "44GB",
                    "cpu": "100GB"
                }
            }
        })

    try:
        import bitsandbytes
        print("âœ“ bitsandbyteså¯ç”¨ï¼Œå¼€å§‹é‡åŒ–åŠ è½½")
        
        # ç­–ç•¥2ï¼š4bité‡åŒ– - æœ€å¿«æœ€ç¨³å®š
        bnb_config_4bit = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_compute_dtype=torch.float16,
            bnb_4bit_use_double_quant=True,
            bnb_4bit_quant_type="nf4"
        )
        
        loading_strategies.append({
            "name": "åŒGPU 4bité‡åŒ–",
            "params": {
                "dtype": torch.float16,
                "device_map": "auto",
                "quantization_config": bnb_config_4bit,
                "cache_dir": model_dir,
                "low_cpu_mem_usage": True,
                "trust_remote_code": True,
                "max_memory": {
                    0: "40GB",
                    1: "40GB",
                    "cpu": "100GB"
                }
            }
        })
        
        # ç­–ç•¥3ï¼š8bité‡åŒ– - æ›´é«˜ç²¾åº¦
        bnb_config_8bit = BitsAndBytesConfig(
            load_in_8bit=True,
            bnb_8bit_compute_dtype=torch.float16
        )
        
        loading_strategies.append({
            "name": "åŒGPU 8bité‡åŒ–",
            "params": {
                "dtype": torch.float16,
                "device_map": "auto",
                "quantization_config": bnb_config_8bit,
                "cache_dir": model_dir,
                "low_cpu_mem_usage": True,
                "trust_remote_code": True,
                "max_memory": {
                    0: "40GB",
                    1: "40GB", 
                    "cpu": "100GB"
                }
            }
        })
        
    except ImportError:
        print("âœ— bitsandbytesä¸å¯ç”¨ï¼Œè¯·å®‰è£…: pip install bitsandbytes")
    
    for strategy in loading_strategies:
        try:
            print(f"\nğŸš€ å°è¯•: {strategy['name']}")
            
            # æ¸…ç†å†…å­˜
            gc.collect()
            torch.cuda.empty_cache()
            
            model = AutoModelForCausalLM.from_pretrained(model_name, **strategy['params'])
            
            print(f"âœ… æˆåŠŸåŠ è½½: {strategy['name']}")
            
            # æ˜¾ç¤ºåŠ è½½ç»“æœ
            current_mem = psutil.virtual_memory().percent
            print(f"ğŸ“Š å½“å‰ç³»ç»Ÿå†…å­˜ä½¿ç”¨: {current_mem:.1f}%")
            
            if torch.cuda.is_available():
                for i in range(torch.cuda.device_count()):
                    allocated = torch.cuda.memory_allocated(i) / 1e9
                    total = torch.cuda.get_device_properties(i).total_memory / 1e9
                    print(f"ğŸ“Š GPU {i}: {allocated:.1f}GB / {total:.1f}GB ({allocated/total*100:.1f}%)")
            
            # æ˜¾ç¤ºæ¨¡å‹åˆ†å¸ƒ
            if hasattr(model, 'hf_device_map'):
                gpu_0_layers = sum(1 for k, v in model.hf_device_map.items() if "layers" in k and v == 0)
                gpu_1_layers = sum(1 for k, v in model.hf_device_map.items() if "layers" in k and v == 1)
                cpu_layers = sum(1 for k, v in model.hf_device_map.items() if "layers" in k and v == "cpu")
                
                print(f"ğŸ”§ æ¨¡å‹åˆ†å¸ƒ: GPU0({gpu_0_layers}å±‚) + GPU1({gpu_1_layers}å±‚) + CPU({cpu_layers}å±‚)")
            
            return model, tokenizer
            
        except Exception as e:
            print(f"âŒ {strategy['name']} å¤±è´¥: {str(e)[:200]}...")
            gc.collect()
            torch.cuda.empty_cache()
            continue
    
    raise RuntimeError("ğŸš« æ‰€æœ‰åŠ è½½ç­–ç•¥éƒ½å¤±è´¥äº†")

def generate_response(model, tokenizer, prompt, max_new_tokens=None):
    """ç”Ÿæˆå“åº” - é«˜æ€§èƒ½ç‰ˆ"""
    messages = [{"role": "user", "content": prompt}]
    text = tokenizer.apply_chat_template(
        messages,
        tokenize=False,
        add_generation_prompt=True,
    )
    
    # æ™ºèƒ½è®¾å¤‡æ£€æµ‹
    if hasattr(model, 'hf_device_map') and model.hf_device_map:
        # ä¼˜å…ˆä½¿ç”¨embed_tokensçš„è®¾å¤‡
        input_device = model.hf_device_map.get('model.embed_tokens', 0)
    else:
        input_device = next(model.parameters()).device
    
    model_inputs = tokenizer([text], return_tensors="pt").to(input_device)
    
    if max_new_tokens is None:
        max_new_tokens = 32768 if "Thinking" in model.name_or_path else 16384
    
    print(f"ğŸ¯ å¼€å§‹ç”Ÿæˆ (æœ€å¤§token: {max_new_tokens}, è®¾å¤‡: {input_device})")
    
    # ä¼˜åŒ–çš„ç”Ÿæˆå‚æ•°
    generation_kwargs = {
        **model_inputs,
        "max_new_tokens": max_new_tokens,
        "temperature": 0.7,
        "do_sample": True,
        "top_p": 0.8,
        "repetition_penalty": 1.05,
        "pad_token_id": tokenizer.eos_token_id or tokenizer.pad_token_id,
        "use_cache": True,
    }
    
    with torch.no_grad():
        generated_ids = model.generate(**generation_kwargs)
    
    output_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist()
    
    # è§£æè¾“å‡º
    if "Thinking" in model.name_or_path:
        try:
            index = len(output_ids) - output_ids[::-1].index(151668)
        except ValueError:
            index = 0
        
        thinking_content = tokenizer.decode(output_ids[:index], skip_special_tokens=True).strip()
        content = tokenizer.decode(output_ids[index:], skip_special_tokens=True).strip()
        return thinking_content, content
    else:
        content = tokenizer.decode(output_ids, skip_special_tokens=True)
        return None, content

def main():
    parser = argparse.ArgumentParser(description="Qwen3æ¨¡å‹è°ƒç”¨è„šæœ¬")
    parser.add_argument("--model", type=str, required=True)
    parser.add_argument("--prompt", type=str, required=True)
    parser.add_argument("--max_new_tokens", type=int, default=None)
    
    args = parser.parse_args()
    
    print(f"ğŸš€ æ­£åœ¨åŠ è½½æ¨¡å‹: {args.model}")
    print(f"ğŸ–¥ï¸  æ£€æµ‹åˆ° {torch.cuda.device_count()} ä¸ªGPU")
    
    model, tokenizer = load_model_and_tokenizer(args.model)
    
    thinking_content, content = generate_response(model, tokenizer, args.prompt, args.max_new_tokens)
    
    if thinking_content:
        print("\n" + "="*50)
        print("ğŸ§  THINKING CONTENT")
        print("="*50)
        print(thinking_content)
    
    print("\n" + "="*50)
    print("ğŸ’¬ RESPONSE") 
    print("="*50)
    print(content)

if __name__ == "__main__":
    main()
