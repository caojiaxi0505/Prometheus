#!/usr/bin/env python3
"""
GSM8K å®Œæ•´æ•°æ®é›†è®­ç»ƒè„šæœ¬ - å®Œå…¨å¤åˆ¶æˆåŠŸç‰ˆæœ¬
åŸºäºfix_gradient_training.pyçš„æˆåŠŸç»éªŒï¼Œåªä¿®æ”¹æ•°æ®é›†éƒ¨åˆ†
"""

import os
import torch
import logging
from datasets import load_dataset
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    TrainingArguments,
    Trainer,
    DataCollatorForLanguageModeling,
)
from peft import LoraConfig, get_peft_model, TaskType

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def main():
    """ä¸»å‡½æ•° - å®Œæ•´æ•°æ®é›†è®­ç»ƒ"""
    logger.info("=== GSM8K å®Œæ•´æ•°æ®é›†è®­ç»ƒï¼ˆæˆåŠŸç‰ˆæœ¬å¤åˆ¶ï¼‰===")
    
    # æ¨¡å‹é…ç½®
    model_name = "Qwen/Qwen3-30B-A3B-Instruct-2507"
    output_dir = "./gsm8k_full_finetuned"
    
    # æ¸…ç†GPUå†…å­˜
    torch.cuda.empty_cache()
    
    # åŠ è½½tokenizer
    logger.info("åŠ è½½tokenizer...")
    tokenizer = AutoTokenizer.from_pretrained(
        model_name,
        trust_remote_code=True
    )
    
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    
    # åŠ è½½æ¨¡å‹ï¼ˆä¸ä½¿ç”¨é‡åŒ–ä»¥é¿å…å¤æ‚æ€§ï¼‰
    logger.info("åŠ è½½æ¨¡å‹...")
    model = AutoModelForCausalLM.from_pretrained(
        model_name,
        device_map="auto",
        torch_dtype=torch.float16,
        trust_remote_code=True,
        low_cpu_mem_usage=True
    )
    
    # é…ç½®LoRA - ä½¿ç”¨æˆåŠŸéªŒè¯çš„é…ç½®
    logger.info("é…ç½®LoRA...")
    lora_config = LoraConfig(
        task_type=TaskType.CAUSAL_LM,
        r=16,
        lora_alpha=32,
        lora_dropout=0.1,
        target_modules=["q_proj", "v_proj"],  # åªé€‰æ‹©æœ€åŸºæœ¬çš„æ¨¡å—
        bias="none",
    )
    
    model = get_peft_model(model, lora_config)
    model.print_trainable_parameters()
    
    # éªŒè¯æ¨¡å‹æ¢¯åº¦
    logger.info("éªŒè¯æ¨¡å‹æ¢¯åº¦...")
    model.train()
    
    # åˆ›å»ºç®€å•çš„æµ‹è¯•è¾“å…¥
    test_input = torch.randint(0, min(1000, tokenizer.vocab_size), (1, 10)).to(model.device)
    
    with torch.enable_grad():
        outputs = model(test_input)
        if isinstance(outputs, dict):
            logits = outputs.get('logits')
        else:
            logits = outputs[0] if isinstance(outputs, tuple) else outputs
        
        if logits is not None and logits.requires_grad:
            logger.info("âœ… æ¨¡å‹å¯ä»¥äº§ç”Ÿæ¢¯åº¦")
            
            # è®¡ç®—ç®€å•æŸå¤±
            loss = logits.mean()
            logger.info(f"æµ‹è¯•æŸå¤±: {loss.item():.6f}")
            
            # åå‘ä¼ æ’­
            loss.backward()
            
            # æ£€æŸ¥æ˜¯å¦æœ‰æ¢¯åº¦
            has_grad = False
            for name, param in model.named_parameters():
                if param.grad is not None and param.grad.abs().sum() > 0:
                    has_grad = True
                    logger.info(f"âœ… å‚æ•° {name} æœ‰æ¢¯åº¦")
                    break
            
            if has_grad:
                logger.info("âœ… æ¢¯åº¦éªŒè¯æˆåŠŸ")
            else:
                logger.warning("âš ï¸ æ²¡æœ‰æ‰¾åˆ°æ¢¯åº¦")
        else:
            logger.error("âŒ æ¨¡å‹æ— æ³•äº§ç”Ÿæ¢¯åº¦")
            return False
    
    # åŠ è½½å®Œæ•´æ•°æ®é›† - è¿™æ˜¯å”¯ä¸€ä¿®æ”¹çš„åœ°æ–¹
    logger.info("åŠ è½½å®Œæ•´GSM8Kæ•°æ®é›†...")
    dataset = load_dataset("gsm8k", "main", split="train")  # å®Œæ•´æ•°æ®é›†ï¼Œä¸å†æ˜¯å­é›†
    
    def format_example(example):
        question = example["question"]
        answer = example["answer"]
        return {"text": f"Question: {question}\nAnswer: {answer}"}
    
    # æ ¼å¼åŒ–æ•°æ®
    logger.info("æ ¼å¼åŒ–æ•°æ®...")
    formatted_dataset = dataset.map(
        format_example,
        remove_columns=dataset.column_names,
        desc="æ ¼å¼åŒ–æ•°æ®"
    )
    
    logger.info(f"å®Œæ•´æ•°æ®é›†å¤§å°: {len(formatted_dataset)}")  # åº”è¯¥æ˜¯7473ä¸ªæ ·æœ¬
    
    # å…³é”®çš„tokenåŒ–å‡½æ•° - ä½¿ç”¨return_tensors=Noneé¿å…è®¾å¤‡é—®é¢˜
    def tokenize_function(examples):
        # ä½¿ç”¨return_tensors=Noneï¼Œè®©Trainerå¤„ç†è®¾å¤‡è½¬ç§»
        tokenized = tokenizer(
            examples["text"],
            truncation=True,
            padding="max_length",
            max_length=256,  # ä¿æŒä¸æˆåŠŸç‰ˆæœ¬ç›¸åŒ
            return_tensors=None  # å…³é”®ï¼šä¸æŒ‡å®šå¼ é‡ç±»å‹
        )
        
        # åˆ›å»ºlabels
        tokenized["labels"] = tokenized["input_ids"].copy()
        
        # å¤„ç†pad_token
        if tokenizer.pad_token_id is not None:
            for i in range(len(tokenized["labels"])):
                tokenized["labels"][i] = [
                    label if label != tokenizer.pad_token_id else -100
                    for label in tokenized["labels"][i]
                ]
        
        return tokenized
    
    # TokenåŒ– - å®Œæ•´æ•°æ®é›†
    logger.info("TokenåŒ–å®Œæ•´æ•°æ®é›†...")
    tokenized_dataset = formatted_dataset.map(
        tokenize_function,
        batched=True,
        remove_columns=["text"],
        desc="TokenåŒ–æ•°æ®"
    )
    
    logger.info(f"TokenåŒ–å®Œæˆï¼Œæ•°æ®é›†å¤§å°: {len(tokenized_dataset)}")
    
    # æ•°æ®æ”¶é›†å™¨ - å…³é”®é…ç½®ï¼ˆä¸æˆåŠŸç‰ˆæœ¬å®Œå…¨ç›¸åŒï¼‰
    data_collator = DataCollatorForLanguageModeling(
        tokenizer=tokenizer,
        mlm=False,
        return_tensors="pt"  # è®©æ•°æ®æ”¶é›†å™¨å¤„ç†å¼ é‡
    )
    
    # è®­ç»ƒå‚æ•° - ä¸æˆåŠŸç‰ˆæœ¬å®Œå…¨ç›¸åŒçš„é…ç½®
    training_args = TrainingArguments(
        output_dir=output_dir,
        num_train_epochs=3,  # æ”¹ä¸º3è½®å®Œæ•´è®­ç»ƒ
        per_device_train_batch_size=1,  # å°æ‰¹æ¬¡
        gradient_accumulation_steps=4,  # æ¢¯åº¦ç´¯ç§¯
        learning_rate=3e-4,
        warmup_steps=10,
        max_grad_norm=1.0,
        logging_steps=5,
        save_steps=50,
        save_total_limit=1,
        load_best_model_at_end=False,
        report_to=None,  # ç¦ç”¨å¤–éƒ¨æ—¥å¿—
        fp16=True,
        dataloader_num_workers=1,
        remove_unused_columns=False,
        # å…³é”®ï¼šä¸è®¾ç½®max_stepsï¼Œè®©è®­ç»ƒè·‘å®Œæ•´ä¸ªæ•°æ®é›†
        # å¦‚æœéœ€è¦æµ‹è¯•ï¼Œå¯ä»¥æ·»åŠ ï¼šmax_steps=100
    )
    
    # åˆ›å»ºè®­ç»ƒå™¨
    logger.info("åˆ›å»ºè®­ç»ƒå™¨...")
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=tokenized_dataset,
        data_collator=data_collator,
    )
    
    # å¼€å§‹è®­ç»ƒ
    logger.info("å¼€å§‹å®Œæ•´æ•°æ®é›†è®­ç»ƒ...")
    try:
        trainer.train()
        logger.info("âœ… å®Œæ•´æ•°æ®é›†è®­ç»ƒå®Œæˆï¼")
        
        # ä¿å­˜æ¨¡å‹
        logger.info("ä¿å­˜æ¨¡å‹...")
        model.save_pretrained(output_dir)
        tokenizer.save_pretrained(output_dir)
        
        logger.info(f"ğŸ‰ å®Œæ•´æ•°æ®é›†è®­ç»ƒæˆåŠŸå®Œæˆï¼æ¨¡å‹ä¿å­˜åœ¨: {output_dir}")
        logger.info(f"è®­ç»ƒæ ·æœ¬æ•°: {len(tokenized_dataset)}")
        return True
        
    except Exception as e:
        logger.error(f"âŒ è®­ç»ƒå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    success = main()
    exit(0 if success else 1)