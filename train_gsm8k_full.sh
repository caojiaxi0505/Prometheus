#!/bin/bash
# GSM8K хоМцХ┤цХ░цНощЫЖшонч╗ГшДЪцЬм
# щАВчФиф║ОхПМGPUчОпхвГя╝Мф╜┐чФихоМцХ┤7473ф╕кшонч╗Гца╖цЬм

# шо╛ч╜очОпхвГхПШщЗП - хПМGPUф╝ШхМЦ
export CUDA_VISIBLE_DEVICES=0,1
export HF_ENDPOINT=https://hf-mirror.com
export TOKENIZERS_PARALLELISM=false
export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True
export WANDB_DISABLED=true

echo "ЁЯОп х╝АхзЛGSM8KхоМцХ┤цХ░цНощЫЖшонч╗Г..."
echo "цибхЮЛ: Qwen/Qwen3-30B-A3B-Instruct-2507"
echo "ш╛УхЗ║чЫох╜Х: ./gsm8k_full_finetuned"

# хИЫх╗║ш╛УхЗ║чЫох╜Х
mkdir -p ./gsm8k_full_finetuned
mkdir -p logs
mkdir -p results

# ц╕ЕчРЖGPUхЖЕхнШ
python -c "
import torch
import gc
if torch.cuda.is_available():
    torch.cuda.empty_cache()
    torch.cuda.synchronize()
    gc.collect()
print('GPUхЖЕхнШх╖▓ц╕ЕчРЖ')
"

echo "ЁЯПГ х╝АхзЛхоМцХ┤цХ░цНощЫЖшонч╗Г..."
WANDB_DISABLED=true python gsm8k_finetune.py \
    --model "Qwen/Qwen3-30B-A3B-Instruct-2507" \
    --output_dir "./gsm8k_full_finetuned" \
    --num_epochs 3 \
    --batch_size 1 \
    --learning_rate 2e-4 \
    --lora_r 16 \
    --max_length 512 \
    2>&1 | tee logs/gsm8k_full_training_$(date +%Y%m%d_%H%M%S).log

echo "тЬЕ хоМцХ┤цХ░цНощЫЖшонч╗ГхоМцИРя╝БцибхЮЛф┐ЭхнШхЬи: ./gsm8k_full_finetuned"

# хПпщАЙя╝Ъш┐РшбМхоМцХ┤шпДф╝░
echo "ЁЯФН х╝АхзЛхоМцХ┤шпДф╝░..."
python gsm8k_inference.py \
    --base_model "Qwen/Qwen3-30B-A3B-Instruct-2507" \
    --finetuned_model "./gsm8k_full_finetuned" \
    --mode evaluate \
    --output_file "results/gsm8k_full_results_$(date +%Y%m%d_%H%M%S).json" \
    2>&1 | tee logs/gsm8k_full_evaluation_$(date +%Y%m%d_%H%M%S).log

echo "ЁЯОЙ хоМцХ┤цХ░цНощЫЖшонч╗Гц╡БчиЛхоМцИРя╝Б"

echo ""
echo "ЁЯУК шонч╗Гч╗Яшоб:"
echo "- шонч╗Гца╖цЬм: 7473ф╕к (хоМцХ┤GSM8Kшонч╗ГщЫЖ)"
echo "- щкМшпБца╖цЬм: 748ф╕к (GSM8Kц╡ЛшпХщЫЖ)"
echo "- цибхЮЛхПВцХ░: 30.5B (хЯ║чбА) + 6.7M (LoRAхПпшонч╗Г)"
echo "- шонч╗ГщЕНч╜о: хПМGPU, FP16, цвпх║жцгАцЯечВ╣"